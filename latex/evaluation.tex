\documentclass{article}

\usepackage{listings}
\usepackage{booktabs}
\newcommand\xor{\wedge}

\usepackage{microtype}
\usepackage{inconsolata}
%\usepackage{minted}
%\usepackage[scaled]{beramono}
\usepackage{tikz}
\usetikzlibrary{arrows,automata,decorations.markings,positioning,calc}

\lstdefinestyle{customc}{%
  belowcaptionskip=1\baselineskip,
  breaklines=true,
  xleftmargin=\parindent,
  language=C,
  showstringspaces=false,
  basicstyle=\small\ttfamily,
  keywordstyle=\bfseries\color{green!40!black},
  numberstyle=\tiny,
  stepnumber=2, numbersep=5pt
  commentstyle=\itshape\color{purple!40!black},
  identifierstyle=\bfseries\color{black},
  stringstyle=\color{orange},
   morekeywords={uint64_t,uint32_t,__m256i,__m128i,UINT64_C},
}
\lstset{escapechar=@,style=customc}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{url}
\usepackage{underscore}
\usepackage{siunitx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
%%% you can use the following comments for nice comments:
\newcommand{\daniel}[1]{\todo[backgroundcolor=blue!20!white]{#1}}
\newcommand{\danielinline}[1]{\todo[inline,backgroundcolor=blue!20!white]{#1}}
%%% end of nice comments
\usepackage{graphicx}


%%%%%%%%%%% Title of the paper
\title{Experimental evaluation of integer hash functions}
\author{}

\begin{document}
\maketitle

\begin{abstract}
Hash functions are ubiquitous in software. Some data structures, such as hash tables with linear probing under high load factors, require high-quality hash functions. A common strategy is to first hash the keys down to a machine word, before applying a well-tested hash function on this word. We review experimentally several of these candidate hash functions on recent 64-bit Intel processors. We find that \ldots [to be completed]
\end{abstract}
\lstset{escapechar=@,style=customc}

%http://burtleburtle.net/bob/hash/integer.html

Most programming languages provide hash functions and hash tables. A hash function maps keys to fixed-length values. For example, most C++ implementations have hash functions (\texttt{std::hash}) producing 64-bit values on 64-bit systems.

We expect data objects to be mapped
evenly over the possible hash values. 
Many hash functions are invertible, meaning that, for example, they hash  $2^{64}$~values to all possible $2^{64}$~values. We can generalize this analysis: given a hash function from $X$ to $Y$, we say that it is \emph{$K$-regular} if $h(x) = y$ is true for at most $K \lceil |Y| / |X| \rceil $~values $x\in X$ given a fixed $y\in Y$.

(present linear probing here and explain why it can be terrible under a bad hash function.)

One popular requirement inspired by cryptography is the ``avalanche effect'' meaning that if the Hamming distance between $x$ and $y$ is 1, then the probability that any one given bit differs between  $h(x)$ and $h(y)$ is, on average, 50\% (picking $x$ and $y$ at random). We cannot achieve both invertibility and the avalanche effect.\footnote{\url{https://github.com/lemire/Code-used-on-Daniel-Lemire-s-blog/blob/master/extra/bitmixing/bruteforce.py}}

In what follows, we do not consider cryptographic applications where, for example, we might require hash functions to be difficult to invert. We also do not pursue security concerns where, for example, an adversary might attempt to degrade the performance of the system by choosing ``bad'' keys. 


In the paper, for each presented hash function, discuss invertibility.





\section{Random hashing}


In random hashing, 
we pick a hash function  at random in a family of
hash functions.  A family  is
  \emph{universal}~\cite{carter1979universal} if
 $P\left (h(x)=h(x')\right )\leq 1/2^64$
for any $x,x'$ such that $x \neq x'$.
A family is $k$-wise independent~\cite{lemi:one-pass-journal,Lemire2012604} if given  $k$~distinct values $x_1,x_2, \ldots, x_k$ (with $k>1$), their hash
values are independent: $
P\left (h(x_1)= y_1  \; \land  \; h(x_2)= y_2 \; \land  \; \cdots \; \land  \; h(x_k)= y_k  \right ) =\frac{1}{2^{64k}}$
for any hash values $y,y'$. 

\subsection{Polynomial hashing}
Given a field, polynomial hashing using a degree $k-1$ polynomial can achieve $k$-wise independence~\cite{carter1979universal,Thorup:2004:TBH:982792.982884} by the fundamental theorem of algebra.

The hash functions have the form $h(x)=a_{k} x^{k} + \cdots + a_{1} x + a_0$ with the coefficients $a_1, \ldots, a_k$ picked at random in the field. Such a function can be computed efficiently using Horner's rule.

There are two convenient ways to do computations in a field on modern computer. You either do the computations modulo a prime number, or you can use the carryless or polynomial multiplication available on most commodity processors (x64, ARM, POWER).

Polynomial hashing is $k-1$-regular. So though higher degree polynomials achieve better independence, they have weaker regularity.


\subsection{Zobrist hashing}


Zobrist hashing~\cite{zobrist1970, zobrist1990new,thorup2012tabulation,Patrascu:2012:PST:2220357.2220361} is 3-wise independent. It is defined as follows. Pick $w$ that divides 64 (such as $w=8$ or $w=16$).
Consider the family $\mathcal{F}$ of all possible functions $2^w \to [0,2^64)$. There are ${2^64}^{2^w}$ such functions, so that they can each be represented using $2^w 64$~bits. Pick $64/w$~functions $f_1, f_2, \ldots, f_{64/w}$. The total memory usage is $2^w 64^2/w$~bits. 
Given a 64-bit word $X$, we can break it into $64/w$ $w$-bit subwords $X_1,X_2, \ldots, X_{64/w}$, 
the hash function is given by $ f_1(X_1) \xor \cdots \xor  f_{64/w}(X_{64/w})$ where $\xor{}$ is the bitwise exclusive or. Though Zobrist hashing offers strong universality, it may require a lot of memory. Setting aside the issue of cache misses, current x64 processors cannot sustain more than two memory loads per cycle which puts an upper bound on the speed of Zobrist hashing.

Zobrist hashing functions are not invertible.

\section{Deterministic hash function}

A deterministic hash function does not rely on randomness.
Given any deterministic hash function, it is always possible to prepare a set of keys where, for example, collisions are frequent. However, if we do not face an adversary capable of determining the values to be hashed, we might be able to  safely use deterministic hash functions. Note that deterministic hash functions can be compose with random hash functions for added security.

\subsection{MurmurHash}

Popular bit-mixing function (or ``finalizers``) were designed by Austin	Appleby for the MurmurHash family\footnote{\url{https://github.com/aappleby/smhasher/wiki/MurmurHash3}}:


\begin{lstlisting}
h ^= h >> 33;
h *= 0xff51afd7ed558ccd;
h ^= h >> 33;
h *= 0xc4ceb9fe1a85ec53;
h ^= h >> 33;
\end{lstlisting}

It is invertible and was determined, empirically, to have an almost perfect avalanche effect.

\subsection{Koloboke}

Vigna in his fastutil library uses the following bit mixing functions which he attributes to  Roman Leventov's Koloboke


\begin{lstlisting}
public final static long mix( final long x ) {
		long h = x * LONG_PHI;
		h ^= h >>> 32;
		return h ^ (h >>> 16);
}
\end{lstlisting}

where LONG\_PHI is the ``golden ratio`` (0x9E3779B97F4A7C15 or $2^64 (\sqrt{5}-1)/2$).


\section{Experiments}



Hash functions we should include:
\begin{itemize}
\item the identity hash function? 
\item Zobrist (as promoted by Thorup, so it is what I call WZob64)
\item  the Appleby's murmurhash64 bit-mixing hash function (non random)
\item Leventov (non random)
\item ClLinear64
\item TCWLinear64
\item ClCubic64
\item TCWCubic64~\cite{Thorup:2004:TBH:982792.982884}
\item \ldots 
\end{itemize}



There are various ways to cast an integer down to a range (The postprocessing step). You can assume that the range fits a power of two (convenient) and just mask everything but the last few bits. Or you can shift right the value and effectively use the most significant bits.

If the range is not a power of two, you can use a modulo reduction, but that's painful.  We can instead use a multiplication followed by a shift.\footnote{\url{http://lemire.me/blog/2016/06/27/a-fast-alternative-to-the-modulo-reduction/}} 

\subsection{Speed}

Use recent Intel processor (skylake).

Given an array of $x$ keys, how fast can we hash them? Some schemes like Zobrist greatly benefit (due to data locality) when we hash long sequences of keys, as the random bits remain in cache.



\subsection{Random keys}

This is probably the least interesting case, but it should be included.

Assess number of collisions.
Assess average probing distance, 
maximum average probing distance, 
average maximum probing distance, 
speed in a hash map under linear probing with different load factors and number of keys. Make sure that as the number of keys increase the load factor remains fixed.

\subsection{Geometric Sequences}

We want to check the hash functions with keys that are geometric sequences. We want to test a wide range of ``gaps'' and starting points to see if it exposes problems. 

Refer to appendix~A from~\cite{Patrascu:2012:PST:2220357.2220361} for motivation.

Shuffle the keys if needed?

Assess number of collisions.
Assess average probing distance, 
maximum average probing distance, 
average maximum probing distance, 
speed in a hash map under linear probing with different load factors and number of keys.  Make sure that as the number of keys increase the load factor remains fixed.

\subsection{Reversed-bit Geometric Sequences}

Same as geometric sequences, but reverse the bit and or byte order.

\section{Conclusion}

We conclude that this paper was too long.

\bibliographystyle{plain}
\bibliography{hashing} 



\end{document}
